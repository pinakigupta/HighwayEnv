{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "import os\n",
    "import multiprocessing\n",
    "from enum import Enum\n",
    "import json\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from torch import FloatTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "from models.gail import GAIL\n",
    "from generate_expert_data import collect_expert_data, postprocess\n",
    "from forward_simulation import make_configure_env, append_key_to_dict_of_dict, simulate_with_model\n",
    "from sb3_callbacks import CustomCheckpointCallback, CustomMetricsCallback, CustomCurriculamCallback\n",
    "from utilities import extract_post_processed_expert_data, write_module_hierarchy_to_file, DefaultActorCriticPolicy, CustomDataset, CustomExtractor, retrieve_gail_agents\n",
    "import warnings\n",
    "from imitation.algorithms import bc\n",
    "from python_config import sweep_config, env_kwargs\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from imitation.algorithms import bc\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from highway_env.envs.common.action import DiscreteMetaAction\n",
    "ACTIONS_ALL = DiscreteMetaAction.ACTIONS_ALL\n",
    "\n",
    "\n",
    "class TrainEnum(Enum):\n",
    "    RLTRAIN = 0\n",
    "    RLDEPLOY = 1\n",
    "    IRLTRAIN = 2\n",
    "    IRLDEPLOY = 3\n",
    "    EXPERT_DATA_COLLECTION =4\n",
    "    BC = 5\n",
    "    BCDEPLOY = 6\n",
    "\n",
    "train = TrainEnum.BC\n",
    "\n",
    "\n",
    "\n",
    "attention_network_kwargs = dict(\n",
    "    # in_size=5*15,\n",
    "    embedding_layer_kwargs={\"in_size\": 7, \"layer_sizes\": [64, 64], \"reshape\": False},\n",
    "    attention_layer_kwargs={\"feature_size\": 64, \"heads\": 2},\n",
    "    # num_layers = 3,\n",
    ")\n",
    "\n",
    "def timenow():\n",
    "    return datetime.now().strftime(\"%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==================================\n",
    "#        Main script  20 \n",
    "# ==================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "        features_extractor_class=CustomExtractor,\n",
    "        features_extractor_kwargs=attention_network_kwargs,\n",
    "    )\n",
    "\n",
    "if False:\n",
    "    optimal_gail_agent, final_gail_agent = retrieve_gail_agents(\n",
    "                                                                # env= make_configure_env(**env_kwargs).unwrapped, # need only certain parameters\n",
    "                                                                artifact_version='trained_model_directory:v2'\n",
    "                                                                )\n",
    "    reward_oracle = final_gail_agent.d\n",
    "    env_kwargs.update({'reward_oracle':reward_oracle})\n",
    "\n",
    "WARM_START = False\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "month = now.strftime(\"%m\")\n",
    "day = now.strftime(\"%d\")\n",
    "expert_data_file='expert_data_relative.h5'\n",
    "validation_data_file = 'expert_data_rel_val.h5'\n",
    "zip_filename = '../expert_data.zip'\n",
    "n_cpu =  multiprocessing.cpu_count()\n",
    "device = torch.device(\"cpu\")\n",
    "extract_path = \"../data/expert_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/HighwayEnv/scripts\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assuming a zip file containing train data files and validaiton data files\n",
    "# populate hyper parameters and config params, say prepare experiment\n",
    "# Extract Datasets\n",
    "# Load Dataloaders\n",
    "# Create Trainer\n",
    "# Train data for n epochs\n",
    "# Validate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(env, policy, **kwargs):\n",
    "    return       bc.BC(\n",
    "                        observation_space=env.observation_space,\n",
    "                        action_space=env.action_space,\n",
    "                        demonstrations=None, #training_transitions,\n",
    "                        rng=np.random.default_rng(),\n",
    "                        batch_size=kwargs['batch_size'],\n",
    "                        device = device,\n",
    "                        policy=policy\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(zip_filename, **kwargs):\n",
    "    # Extract the HDF5 files from the zip archive\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as archive:\n",
    "        archive.extractall(extract_path)\n",
    "\n",
    "    # Extract the names of the HDF5 files from the zip archive\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as archive:\n",
    "        hdf5_train_file_names = [os.path.join(extract_path, name) \n",
    "                                    for name in archive.namelist() \n",
    "                                    if name.endswith('.h5') and \"train\" in name]\n",
    "        hdf5_val_file_names = [os.path.join(extract_path, name) \n",
    "                                    for name in archive.namelist() \n",
    "                                    if name.endswith('.h5') and \"val\" in name]            \n",
    "\n",
    "    # Create separate datasets for each HDF5 file\n",
    "    train_datasets = [CustomDataset(hdf5_name, device) for hdf5_name in hdf5_train_file_names[:2]]\n",
    "    # val_datasets = [CustomDataset(hdf5_name, device) for hdf5_name in hdf5_val_file_names]\n",
    "    \n",
    "    # custom_dataset = CustomDataset(expert_data_file, device=device)\n",
    "    train_data_loaders = [DataLoader(\n",
    "                                dataset, \n",
    "                                batch_size=kwargs['batch_size'], \n",
    "                                shuffle=True,\n",
    "                                drop_last=True,\n",
    "                                num_workers=n_cpu,\n",
    "                                pin_memory=True\n",
    "                            ) for dataset in train_datasets]\n",
    "    return train_data_loaders, hdf5_train_file_names, hdf5_val_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(trainer, data_loaders, **training_kwargs):\n",
    "    for epoch in range(training_kwargs['num_epochs']):\n",
    "        for data_loader in data_loaders:\n",
    "            trainer.set_demonstrations(data_loader)\n",
    "            trainer.train(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation_metrics(bc_trainer, hdf5_train_file_names, hdf5_val_file_names, **training_kwargs):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    # Iterate through the validation data and make predictions\n",
    "    with torch.no_grad():\n",
    "        for val_data_file in hdf5_val_file_names[:2]:\n",
    "            val_obs, val_acts, val_dones = extract_post_processed_expert_data(val_data_file)\n",
    "            predicted_labels.extend([bc_trainer.policy.predict(obs)[0] for obs in val_obs])\n",
    "            true_labels.extend(val_acts)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average=None)\n",
    "    recall = recall_score(true_labels, predicted_labels, average=None)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(\"Accuracy:\", accuracy, np.mean(accuracy))\n",
    "    print(\"Precision:\", precision, np.mean(precision))\n",
    "    print(\"Recall:\", recall, np.mean(recall))\n",
    "    print(\"F1 Score:\", f1, np.mean(f1))\n",
    "\n",
    "\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for val_data_file in hdf5_train_file_names[:2]:\n",
    "            val_obs, val_acts, val_dones = extract_post_processed_expert_data(val_data_file)\n",
    "            predicted_labels.extend([bc_trainer.policy.predict(obs)[0] for obs in val_obs])\n",
    "            true_labels.extend(val_acts)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average=None)\n",
    "    recall = recall_score(true_labels, predicted_labels, average=None)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"--------  Training data metrics for reference---------------\")\n",
    "    print(\"Accuracy:\", accuracy, np.mean(accuracy))\n",
    "    print(\"Precision:\", precision,  np.mean(precision))\n",
    "    print(\"Recall:\", recall, np.mean(recall))\n",
    "    print(\"F1 Score:\", f1, np.mean(recall))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    class_labels = [ ACTIONS_ALL[idx] for idx in range(len(ACTIONS_ALL))]\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()    \n",
    "\n",
    "    plt.savefig(training_kwargs['plot_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'grid', 'metric': {'name': 'episode_reward', 'goal': 'maximize'}, 'parameters': {'duration': {'values': [40]}, 'gae_gamma': {'values': [0.995]}, 'discrm_lr': {'values': [0.001]}, 'batch_size': {'values': [128]}, 'num_epochs': {'values': [3]}}}\n"
     ]
    }
   ],
   "source": [
    "from python_config import sweep_config\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\n",
    "        \"name\": \"episode_reward\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"duration\": {\n",
    "            \"values\": [40]  # Values for the \"duration\" field to be swept\n",
    "        },\n",
    "        \"gae_gamma\": {\n",
    "            \"values\": [0.995]  # Values for the \"duration\" field to be swept\n",
    "        }, \n",
    "        \"discrm_lr\": {\n",
    "            \"values\": [0.001]  # Values for the \"duration\" field to be swept\n",
    "        },              \n",
    "        \"batch_size\": {\n",
    "            \"values\": [32, 64, 128]  # Values for the \"duration\" field to be swept\n",
    "        }, \n",
    "        \"num_epochs\": {\n",
    "            \"values\": [10, 25, 50]  # Values for the \"duration\" field to be swept\n",
    "        },    \n",
    "    }\n",
    "}\n",
    "print(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: bbd4yq7e\n",
      "Sweep URL: https://wandb.ai/gpinaki/BC_1/sweeps/bbd4yq7e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zivoamsz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdiscrm_lr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tduration: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_gamma: 0.995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 3\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/HighwayEnv/scripts/wandb/run-20230821_080221-zivoamsz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gpinaki/BC_1/runs/zivoamsz' target=\"_blank\">radiant-sweep-1</a></strong> to <a href='https://wandb.ai/gpinaki/BC_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/gpinaki/BC_1/sweeps/bbd4yq7e' target=\"_blank\">https://wandb.ai/gpinaki/BC_1/sweeps/bbd4yq7e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gpinaki/BC_1' target=\"_blank\">https://wandb.ai/gpinaki/BC_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/gpinaki/BC_1/sweeps/bbd4yq7e' target=\"_blank\">https://wandb.ai/gpinaki/BC_1/sweeps/bbd4yq7e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gpinaki/BC_1/runs/zivoamsz' target=\"_blank\">https://wandb.ai/gpinaki/BC_1/runs/zivoamsz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config  {'batch_size': 128, 'discrm_lr': 0.001, 'duration': 40, 'gae_gamma': 0.995, 'num_epochs': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 128      |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00161 |\n",
      "|    entropy        | 1.61     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 129      |\n",
      "|    loss           | 1.61     |\n",
      "|    neglogp        | 1.61     |\n",
      "|    prob_true_act  | 0.2      |\n",
      "|    samples_so_far | 128      |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "139batch [00:02, 60.95batch/s]]\n",
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 128      |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00143 |\n",
      "|    entropy        | 1.43     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 134      |\n",
      "|    loss           | 1.42     |\n",
      "|    neglogp        | 1.42     |\n",
      "|    prob_true_act  | 0.272    |\n",
      "|    samples_so_far | 128      |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "149batch [00:02, 63.53batch/s]]\n"
     ]
    }
   ],
   "source": [
    "if train == TrainEnum.BC:\n",
    "    env = make_configure_env(**env_kwargs)\n",
    "    state_dim = env.observation_space.high.shape[0]*env.observation_space.high.shape[1]\n",
    "    action_dim = env.action_space.n\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    policy = DefaultActorCriticPolicy(env, device)\n",
    "    project_name = f\"BC_1\"\n",
    "\n",
    "    \n",
    "    run_name = f\"sweep_{month}{day}_{timenow()}\"\n",
    "    sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "\n",
    "    def train_sweep(env, policy, config=None):\n",
    "        train_kwargs = {\n",
    "                        \"batch_size\" : 128,\n",
    "                        \"num_epochs\" : 3,\n",
    "                    }\n",
    "        with wandb.init(\n",
    "                            project=project_name, \n",
    "                            config=config,\n",
    "                            # magic=True,\n",
    "                        ) as run:\n",
    "            config = run.config\n",
    "            print(\"config \", config)\n",
    "            run.name = f\"sweep_{month}{day}_{timenow()}\"\n",
    "            name = \"\"\n",
    "            for key, value in config.items():\n",
    "                # Discard first 3 letters from the key\n",
    "                key = key[3:]\n",
    "\n",
    "                # Format the float value\n",
    "                value_str = \"{:.2f}\".format(value).rstrip('0').rstrip('.') if value and '.' in str(value) else str(value)\n",
    "\n",
    "                # Append the formatted key-value pair to the name\n",
    "                name += f\"{key}_{value_str}_\"\n",
    "                # Save the plot as an image\n",
    "            plot_path = f\"plot_{name}.png\"    \n",
    "            train_kwargs['batch_size'] = config.batch_size\n",
    "            train_kwargs['num_epochs'] = config.num_epochs\n",
    "            bc_trainer = create_trainer(env, policy, **train_kwargs)\n",
    "            train_data_loaders, hdf5_train_file_names, hdf5_val_file_names = create_dataloaders(zip_filename, **train_kwargs)\n",
    "            _train(bc_trainer, train_data_loaders, **train_kwargs)\n",
    "            calculate_validation_metrics(bc_trainer, hdf5_train_file_names, hdf5_val_file_names, plot_path=plot_path)\n",
    "\n",
    "            os.makedirs(\"models_archive\", exist_ok=True)\n",
    "            torch.save(bc_trainer, 'models_archive/BC_agent.pth') \n",
    "\n",
    "            # Log the model as an artifact in wandb\n",
    "            artifact = wandb.Artifact(\"trained_model_directory\", type=\"model_directory\")\n",
    "            artifact.add_dir(\"models_archive\")\n",
    "            run.log_artifact(artifact)\n",
    "            # Log the saved plot to WandB\n",
    "            # run.log({f\"plot_{name}\": wandb.Image(plot_path)})\n",
    "                    \n",
    "\n",
    "\n",
    "    wandb.agent(\n",
    "                    sweep_id=sweep_id, \n",
    "                    function=lambda: train_sweep(env, policy)\n",
    "                )\n",
    "\n",
    "    wandb.finish()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
